# -*- coding: utf-8 -*-
"""Copy of final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JsZAdNd67Fcn-S5prbt1w33R4wxE_9ep
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
from sklearn.preprocessing import LabelEncoder
from tqdm import tqdm_notebook as tqdm
# %matplotlib inline

"""## Data loading """

application_train = pd.read_csv("/content/drive/MyDrive/Home Credit/Data/application_train.csv")
application_test = pd.read_csv("/content/drive/MyDrive/Home Credit/Data/application_test.csv")

# pos_cash = pd.read_csv("/content/drive/MyDrive/Home Credit/Data/POS_CASH_balance.csv")
# installments = pd.read_csv("/content/drive/MyDrive/Home Credit/Data/installments_payments.csv")
# credit_df = pd.read_csv("/content/drive/MyDrive/Home Credit/Data/credit_card_balance.csv");

# b=pd.read_csv("/content/drive/MyDrive/Home Credit/Data/bureau.csv")
# bur=pd.read_csv("/content/drive/MyDrive/Home Credit/Data/bureau_balance.csv")
# prev=pd.read_csv("/content/drive/MyDrive/Home Credit/Data/previous_application.csv")

print("application_train.shape:",application_train.shape)
print("application_test.shape :",application_test.shape)

train_id = application_train["SK_ID_CURR"]
train_target = application_train["TARGET"]
test_id = application_test["SK_ID_CURR"]

application_train.head()

application_test.head()

"""we have one extra column in the application_train data , i.e TARGET """

application_train['TARGET'].value_counts()

fig = plt.figure(figsize =(15, 5)) 
plt.subplot(1,2,1)
plt.pie(application_train["TARGET"].value_counts(),labels = ["TARGET=0","TARGET=1"],autopct='%1.2f%%') 

plt.subplot(1,2,2)
sns.countplot(x="TARGET",palette ="Set2",data=application_train)
plt.tight_layout()
plt.show()

"""Imbalanced dataset"""

application_train.dtypes.value_counts()

obj_type  = application_train.dtypes[application_train.dtypes=='object'].index
float_type  = application_train.dtypes[application_train.dtypes=='float64'].index
int_type  = application_train.dtypes[application_train.dtypes=='int64'].index

def missing_data(data):
    total = data.isnull().sum().sort_values(ascending = False)
    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)
    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])

"""# Handling categorical data """

print(obj_type)

label_list = []
one_hot_list = []
drop_list = []

application_train[obj_type].head()

"""Missing Values in categorical data"""

missing_data(application_train[obj_type])

application_train["CODE_GENDER"].value_counts()

application_train['CODE_GENDER'].replace('XNA','F', inplace=True)

fig = plt.figure(figsize =(15, 5)) 
plt.subplot(1,2,1)
plt.pie(application_train["CODE_GENDER"].value_counts(),labels = ["Female","Male"],autopct='%1.2f%%') 

plt.subplot(1,2,2)
sns.countplot(x="CODE_GENDER",hue="TARGET",palette ="Set2",data=application_train)
plt.tight_layout()
plt.show()

"""Observation : Male having difficulty in repaying is high compared to Female from the above graph
More No of Female Applicants than Male Applicants.
"""

def plot_hist(col):
    plt.suptitle(col, fontsize=30)
    
    application_train.loc[application_train['TARGET'] == 0, col].hist( )
    application_train.loc[application_train['TARGET'] == 1, col].hist( )
    plt.legend(['TARGET(0)', 'TARGET(1)'])
    plt.show()

plot_hist("CODE_GENDER")

"""# NAME_CONTRACT_TYPE"""

fig = plt.figure(figsize =(15, 5)) 
plt.subplot(1,2,1)
plt.pie(application_train["NAME_CONTRACT_TYPE"].value_counts(),labels = ["Cash_Loans","Revolving_Loans"],autopct='%1.2f%%') 

plt.subplot(1,2,2)
sns.countplot(x="NAME_CONTRACT_TYPE",hue="TARGET",palette ="Set2",data=application_train)
plt.tight_layout()
plt.show()

"""Cash Loans are More than Revolving loans .

## FLAG_OWN_CAR
"""

fig = plt.figure(figsize =(15, 3)) 
plt.subplot(1,2,1)
plt.pie(application_train["FLAG_OWN_CAR"].value_counts(),labels = ["YES","NO"],autopct='%1.2f%%') 

plt.subplot(1,2,2)
sns.countplot(x="FLAG_OWN_CAR",hue="TARGET",palette ="Set2",data=application_train)
plt.tight_layout()
plt.show()

"""## FLAG_OWN_REALTY"""

fig = plt.figure(figsize =(15, 3)) 
plt.subplot(1,2,1)
plt.pie(application_train["FLAG_OWN_REALTY"].value_counts(),labels = ["YES","NO"],autopct='%1.2f%%') 

plt.subplot(1,2,2)
sns.countplot(x="FLAG_OWN_REALTY",hue="TARGET",palette ="Set2",data=application_train)
plt.tight_layout()
plt.show()

"""## NAME_EDUCATION_TYPE"""

application_train["NAME_EDUCATION_TYPE"].value_counts().index

fig = plt.figure(figsize =(15, 3)) 
plt.subplot(1,2,1)
plt.pie(application_train["NAME_EDUCATION_TYPE"].value_counts(),labels =['Secondary / secondary special', 'Higher education',
       'Incomplete higher', 'Lower secondary', 'Academic degree'],autopct='%1.2f%%') 

plt.subplot(1,2,2)
sns.countplot(x="NAME_EDUCATION_TYPE",hue="TARGET",palette ="Set2",data=application_train)
plt.tight_layout()
plt.show()

"""## NAME_TYPE_SUITE"""

application_train["NAME_TYPE_SUITE"].value_counts().index

fig = plt.figure(figsize =(15, 5)) 
plt.subplot(1,2,1)
plt.pie(application_train["NAME_TYPE_SUITE"].value_counts(),labels =['Unaccompanied', 'Family', 'Spouse, partner', 'Children', 'Other_B',
       'Other_A', 'Group of people'],autopct='%1.2f%%') 

plt.subplot(1,2,2)
sns.countplot(x="NAME_TYPE_SUITE",hue="TARGET",palette ="Set2",data=application_train)
plt.tight_layout()
plt.show()

"""## NAME_INCOME_TYPE"""

l=application_train["NAME_INCOME_TYPE"].value_counts()
l

fig = plt.figure(figsize =(20, 5)) 
plt.subplot(1,2,1)
plt.pie(application_train["NAME_INCOME_TYPE"].value_counts(),labels =l.index,autopct='%1.2f%%') 

plt.subplot(1,2,2)
sns.countplot(x="NAME_INCOME_TYPE",hue="TARGET",palette ="Set2",data=application_train)
plt.tight_layout()
plt.show()

"""## NAME_FAMILY_STATUS"""

l=application_train["NAME_FAMILY_STATUS"].value_counts()
l

fig = plt.figure(figsize =(15, 5)) 
plt.subplot(1,2,1)
plt.pie(application_train["NAME_FAMILY_STATUS"].value_counts(),labels =l.index,autopct='%1.2f%%') 

plt.subplot(1,2,2)
sns.countplot(x="NAME_FAMILY_STATUS",hue="TARGET",palette ="Set2",data=application_train)
plt.tight_layout()
plt.show()

"""## NAME_HOUSING_TYPE"""

l= application_train["NAME_HOUSING_TYPE"].value_counts()
l

fig = plt.figure(figsize =(20, 5)) 
plt.subplot(1,2,1)
plt.pie(application_train["NAME_HOUSING_TYPE"].value_counts(),labels =l.index,autopct='%1.2f%%') 

plt.subplot(1,2,2)
sns.countplot(x="NAME_HOUSING_TYPE",hue="TARGET",palette ="Set2",data=application_train)
plt.tight_layout()
plt.show()

"""## OCCUPATION_TYPE"""

l=application_train["OCCUPATION_TYPE"].value_counts()
l

"""## WEEKDAY_APPR_PROCESS_START


"""

l=application_train["WEEKDAY_APPR_PROCESS_START"].value_counts()
l

fig = plt.figure(figsize =(20, 5)) 
plt.subplot(1,2,1)
plt.pie(application_train["WEEKDAY_APPR_PROCESS_START"].value_counts(),labels =l.index,autopct='%1.2f%%') 

plt.subplot(1,2,2)
sns.countplot(x="WEEKDAY_APPR_PROCESS_START",hue="TARGET",palette ="Set2",data=application_train)
plt.tight_layout()
plt.show()

"""## ORGANIZATION_TYPE"""

l=application_train["ORGANIZATION_TYPE"].value_counts()
l

"""## FONDKAPREMONT_MODE"""

l=application_train["FONDKAPREMONT_MODE"].value_counts()
l

fig = plt.figure(figsize =(20, 5)) 
plt.subplot(1,2,1)
plt.pie(application_train["FONDKAPREMONT_MODE"].value_counts(),labels =l.index,autopct='%1.2f%%') 

plt.subplot(1,2,2)
sns.countplot(x="FONDKAPREMONT_MODE",hue="TARGET",palette ="Set2",data=application_train)
plt.tight_layout()
plt.show()

"""## HOUSETYPE_MODE"""

l=application_train["HOUSETYPE_MODE"].value_counts()
l

fig = plt.figure(figsize =(15, 5)) 
plt.subplot(1,2,1)
plt.pie(application_train["HOUSETYPE_MODE"].value_counts(),labels =l.index,autopct='%1.2f%%') 

plt.subplot(1,2,2)
sns.countplot(x="HOUSETYPE_MODE",hue="TARGET",palette ="Set2",data=application_train)
plt.tight_layout()
plt.show()

"""## WALLSMATERIAL_MODE """

l=application_train["WALLSMATERIAL_MODE"].value_counts()
l

fig = plt.figure(figsize =(15, 5)) 
plt.subplot(1,2,1)
plt.pie(application_train["WALLSMATERIAL_MODE"].value_counts(),labels =l.index,autopct='%1.2f%%') 

plt.subplot(1,2,2)
sns.countplot(x="WALLSMATERIAL_MODE",hue="TARGET",palette ="Set2",data=application_train)
plt.tight_layout()
plt.show()

"""## EMERGENCYSTATE_MODE"""

l=application_train["EMERGENCYSTATE_MODE"].value_counts()
l

fig = plt.figure(figsize =(10, 5)) 
plt.subplot(1,2,1)
plt.pie(application_train["EMERGENCYSTATE_MODE"].value_counts(),labels =l.index,autopct='%1.2f%%') 

plt.subplot(1,2,2)
sns.countplot(x="EMERGENCYSTATE_MODE",hue="TARGET",palette ="Set2",data=application_train)
plt.tight_layout()
plt.show()

obj_type

label_list = ['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR','FLAG_OWN_REALTY', 'ORGANIZATION_TYPE']
one_hot_list = ['NAME_TYPE_SUITE', 'NAME_INCOME_TYPE','NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS','NAME_HOUSING_TYPE','OCCUPATION_TYPE', 'WEEKDAY_APPR_PROCESS_START','FONDKAPREMONT_MODE','WALLSMATERIAL_MODE']
drop_list = ["HOUSETYPE_MODE","EMERGENCYSTATE_MODE"]

le = LabelEncoder()
for x in label_list:
    le.fit(application_train[x])
    application_train[x] = le.transform(application_train[x]) 
    application_test[x] = le.transform(application_test[x])



application_train.drop(drop_list,axis=1,inplace=True)
application_test.drop(drop_list,axis=1,inplace=True)

train_id = application_train["SK_ID_CURR"]
test_id = application_test["SK_ID_CURR"]
train_target = application_train["TARGET"]

application_train.drop(["SK_ID_CURR","TARGET"],axis=1,inplace=True)
application_test.drop(["SK_ID_CURR"],axis=1,inplace=True)

print(application_train.shape)
print(application_test.shape)

obj_type  = application_train.dtypes[application_train.dtypes=='object'].index

obj_type

application_train = pd.get_dummies(application_train,columns=one_hot_list)
application_test = pd.get_dummies(application_test,columns=one_hot_list)

print(application_train.shape)
print(application_test.shape)

application_train, application_test = application_train.align(application_test, join ='inner', axis = 1)

print(application_train.shape)
print(application_test.shape)

application_test["SK_ID_CURR"] = test_id
application_train["SK_ID_CURR"] = train_id
application_train["TARGET"] = train_target

"""## Handling NUMERICAL DATA"""

application_train[int_type].head()

for x in int_type:
    print(x)

"""## CNT_CHILDREN"""

l=application_train['CNT_CHILDREN'].value_counts()
l

fig = plt.figure(figsize =(10, 5)) 
plt.subplot(1,2,1)
plt.pie(application_train["CNT_CHILDREN"].value_counts(),labels =l.index,autopct='%1.2f%%') 

plt.subplot(1,2,2)
sns.countplot(x="CNT_CHILDREN",hue="TARGET",palette ="Set2",data=application_train)
plt.tight_layout()
plt.show()

"""## DAYS_BIRTH   
Client's age in days at the time of application	,time only relative to the application
"""

application_train['DAYS_BIRTH'].apply(lambda x : -1*x/365).plot.hist()

application_train['DAYS_BIRTH'].apply(lambda x : -1*x/365).describe()

"""## DAYS_EMPLOYED"""

application_train['DAYS_EMPLOYED'].describe()

application_train['DAYS_EMPLOYED'].apply(lambda x : -1*x/365).describe()

"""here we see that max no of days employed showing 1000 years and showing positive . these are outliers."""

application_train['DAYS_EMPLOYED'].apply(lambda x : x/365).plot.hist()

"""So the DAYS_EMPLOYED greater than the 100 years are considered as outliers , we must Delete the Outliers """

application_train['DAYS_EMPLOYED'].apply(lambda x : x/365).value_counts()

"""35869 rows has the Days employed value 1000 years ,changing these rows to nan"""

application_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)
application_test['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)

application_train['DAYS_EMPLOYED'].plot.hist()

"""## FLAG_MOBIL : 
Did client provide mobile phone (1=YES, 0=NO)
"""

l=application_train['FLAG_MOBIL'].value_counts()
l

fig = plt.figure(figsize =(8, 3)) 
plt.subplot(1,2,1)
plt.pie(application_train["FLAG_MOBIL"].value_counts(),labels =l.index) 

# plt.subplot(1,2,2)
# sns.countplot(x="CNT_CHILDREN",hue="TARGET",palette ="Set2",data=application_train)
# plt.tight_layout()
plt.show()

application_train['FLAG_MOBIL'].value_counts()

application_test['FLAG_MOBIL'].value_counts()

"""so droping the column FLAG_MOBIL"""

application_train.drop(['FLAG_MOBIL'],axis=1,inplace=True)
application_test.drop(['FLAG_MOBIL'],axis=1,inplace=True)

"""## FLAG_DOCUMENT_#
The below are the documents may the necessary documents ,They are submitted by all most all, so they do not contribute to any information in prediction 
"""

d=['FLAG_DOCUMENT_2','FLAG_DOCUMENT_4','FLAG_DOCUMENT_6','FLAG_DOCUMENT_7','FLAG_DOCUMENT_9','FLAG_DOCUMENT_10','FLAG_DOCUMENT_11','FLAG_DOCUMENT_12','FLAG_DOCUMENT_13',
   'FLAG_DOCUMENT_14','FLAG_DOCUMENT_15','FLAG_DOCUMENT_16','FLAG_DOCUMENT_17','FLAG_DOCUMENT_18','FLAG_DOCUMENT_19',
   'FLAG_DOCUMENT_20','FLAG_DOCUMENT_21']

#for example :
l=application_train['FLAG_DOCUMENT_4'].value_counts()
l

fig = plt.figure(figsize =(8, 3)) 
plt.subplot(1,2,1)
plt.pie(application_train["FLAG_DOCUMENT_4"].value_counts(),labels =l.index) 
plt.show()

l=application_train['FLAG_DOCUMENT_9'].value_counts()
l

fig = plt.figure(figsize =(8, 3)) 
plt.subplot(1,2,1)
plt.pie(application_train["FLAG_DOCUMENT_9"].value_counts(),labels =l.index) 
plt.show()

application_train.drop(d,axis=1,inplace=True)
application_test.drop(d,axis=1,inplace=True)

print(application_train.shape)
print(application_test.shape)

"""## Handling Float"""

def plot_kde(col):
    plt.suptitle(col, fontsize=30)
    sns.kdeplot(application_train.loc[application_train['TARGET'] == 0, col],label='TARGET=0')
    sns.kdeplot(application_train.loc[application_train['TARGET'] == 1, col],label='TARGET=1')
    plt.legend(['TARGET(0)', 'TARGET(1)'])
    plt.show()

def plot_hist(col):
    plt.suptitle(col, fontsize=30)
    
    application_train.loc[application_train['TARGET'] == 0, col].hist( )
    application_train.loc[application_train['TARGET'] == 1, col].hist( )
    plt.legend(['TARGET(0)', 'TARGET(1)'])
    plt.show()

def kde_hist(col):
    plt.suptitle(col, fontsize=30)
    fig, ax = plt.subplots(1, 2, figsize=(16, 8))
    sns.kdeplot(application_train.loc[application_train['TARGET'] == 0,col], ax=ax[0], label='TARGET(0)')
    sns.kdeplot(application_train.loc[application_train['TARGET'] == 1,col], ax=ax[0], label='TARGET(1)')
    ax[0].set_title('KDE plot')
    ax[1].set_title('Histogram plot')
    application_train.loc[application_train['TARGET'] == 0, col].hist(ax=ax[1])
    application_train.loc[application_train["TARGET"] == 1, col].hist(ax=ax[1])
    ax[1].legend(['TARGET(0)', 'TARGET(1)'])

    plt.show()

"""## AMT_INCOME_TOTAL"""

missing_data(pd.DataFrame(application_train["AMT_INCOME_TOTAL"]))

application_train["AMT_INCOME_TOTAL"].describe()

plt.figure(figsize=(5,5))
sns.boxplot(application_train["AMT_INCOME_TOTAL"])

kde_hist("AMT_INCOME_TOTAL")

"""we can observe that higher income does not have any problem in repaying the loan

## AMT_CREDIT : Credit amount of the loan
"""

missing_data(pd.DataFrame(application_train["AMT_CREDIT"]))

plot_kde("AMT_CREDIT")

"""## AMT_ANNUITY : Loan annuity"""

missing_data(pd.DataFrame(application_train["AMT_ANNUITY"]))

plot_kde("AMT_ANNUITY")

plt.figure(figsize=(5,5))
sns.boxplot(application_train["AMT_ANNUITY"])

"""## OWN_CAR_AGE  :Age of client's car"""

missing_data(pd.DataFrame(application_train["OWN_CAR_AGE"]))

application_train["OWN_CAR_AGE"].describe()

plot_kde("OWN_CAR_AGE")

"""considering nan value means no car ,so no age so filling with zero."""

application_train["OWN_CAR_AGE"].fillna(0,inplace=True)
application_test["OWN_CAR_AGE"].fillna(0,inplace=True)

"""## CNT_FAM_MEMBERS : How many family members does client have"""

missing_data(pd.DataFrame(application_train["CNT_FAM_MEMBERS"]))

application_train["CNT_FAM_MEMBERS"].value_counts()

plot_hist("CNT_FAM_MEMBERS")

"""we can observer that more the family members difficulty in paying loan .

## EXT_SOURCE_1 ,EXT_SOURCE_2,EXT_SOURCE_3 : 
Normalized score from external data source
"""

missing_data(pd.DataFrame(application_train[["EXT_SOURCE_1","EXT_SOURCE_2","EXT_SOURCE_3"]]))

# #droping EXT_SOURCE_1
# application_train.drop(["EXT_SOURCE_1"],axis=1,inplace=True)
# application_test.drop(["EXT_SOURCE_1"],axis=1,inplace=True)

kde_hist("EXT_SOURCE_2")

kde_hist("EXT_SOURCE_3")

"""## TOTALAREA_MODE"""

missing_data(pd.DataFrame(application_train["TOTALAREA_MODE"]))

application_train["TOTALAREA_MODE"].describe()

application_train["TOTALAREA_MODE"].fillna(0,inplace=True)
application_test["TOTALAREA_MODE"].fillna(0,inplace=True)

kde_hist("TOTALAREA_MODE")

"""## OBS_30_CNT_SOCIAL_CIRCLE ,OBS_60 _CNT_SOCIAL_CIRCLE
How many observation of client's social surroundings with observable 30 DPD (days past due) default

How many observation of client's social surroundings with observable 60 DPD (days past due) default
"""

missing_data(pd.DataFrame(application_train[["OBS_30_CNT_SOCIAL_CIRCLE","OBS_60_CNT_SOCIAL_CIRCLE"]]))

application_train["OBS_30_CNT_SOCIAL_CIRCLE"].fillna(0,inplace=True)
application_test["OBS_30_CNT_SOCIAL_CIRCLE"].fillna(0,inplace=True)
application_train["OBS_60_CNT_SOCIAL_CIRCLE"].fillna(0,inplace=True)
application_test["OBS_60_CNT_SOCIAL_CIRCLE"].fillna(0,inplace=True)

application_train["OBS_30_CNT_SOCIAL_CIRCLE"].value_counts()

sns.boxplot(application_train["OBS_30_CNT_SOCIAL_CIRCLE"])

application_train["OBS_30_CNT_SOCIAL_CIRCLE"] = application_train["OBS_30_CNT_SOCIAL_CIRCLE"].apply(lambda x: 25 if x>25 else x)
application_test["OBS_30_CNT_SOCIAL_CIRCLE"]= application_test["OBS_30_CNT_SOCIAL_CIRCLE"].apply(lambda x: 25 if x>25 else x)

application_train["OBS_60_CNT_SOCIAL_CIRCLE"] = application_train["OBS_60_CNT_SOCIAL_CIRCLE"].apply(lambda x: 25 if x>25 else x)
application_test["OBS_60_CNT_SOCIAL_CIRCLE"]= application_test["OBS_60_CNT_SOCIAL_CIRCLE"].apply(lambda x: 25 if x>25 else x)

sns.boxplot(application_train["OBS_60_CNT_SOCIAL_CIRCLE"])

"""## DEF_30_CNT_SOCIAL_CIRCLE, DEF_60_CNT_SOCIAL_CIRCLE,
How many observation of client's social surroundings defaulted on 30 (days past due) DPD

How many observation of client's social surroundings defaulted on 60 (days past due) DPD
"""

missing_data(pd.DataFrame(application_train[["DEF_30_CNT_SOCIAL_CIRCLE","DEF_60_CNT_SOCIAL_CIRCLE"]]))

application_train["DEF_30_CNT_SOCIAL_CIRCLE"].fillna(0,inplace=True)
application_test["DEF_30_CNT_SOCIAL_CIRCLE"].fillna(0,inplace=True)
application_train["DEF_60_CNT_SOCIAL_CIRCLE"].fillna(0,inplace=True)
application_test["DEF_60_CNT_SOCIAL_CIRCLE"].fillna(0,inplace=True)

application_train["DEF_30_CNT_SOCIAL_CIRCLE"].value_counts()

application_train["DEF_60_CNT_SOCIAL_CIRCLE"].value_counts()

sns.boxplot(application_train["DEF_30_CNT_SOCIAL_CIRCLE"])

application_train["DEF_30_CNT_SOCIAL_CIRCLE"] = application_train["DEF_30_CNT_SOCIAL_CIRCLE"].apply(lambda x: 5 if x>4 else x)
application_test["DEF_30_CNT_SOCIAL_CIRCLE"]= application_test["DEF_30_CNT_SOCIAL_CIRCLE"].apply(lambda x: 5 if x>4 else x)
application_train["DEF_60_CNT_SOCIAL_CIRCLE"] = application_train["DEF_60_CNT_SOCIAL_CIRCLE"].apply(lambda x: 5 if x>4 else x)
application_test["DEF_60_CNT_SOCIAL_CIRCLE"]= application_test["DEF_60_CNT_SOCIAL_CIRCLE"].apply(lambda x: 5 if x>4 else x)



"""## DAYS_LAST_PHONE_CHANGE :
How many days before application did client change phone
"""

missing_data(pd.DataFrame(application_train["DAYS_LAST_PHONE_CHANGE"]))

application_train["DAYS_LAST_PHONE_CHANGE"].describe()

application_train["DAYS_LAST_PHONE_CHANGE"]=application_train["DAYS_LAST_PHONE_CHANGE"].apply(lambda x: x*-1)
application_test["DAYS_LAST_PHONE_CHANGE"]=application_test["DAYS_LAST_PHONE_CHANGE"].apply(lambda x: x*-1)

kde_hist("DAYS_LAST_PHONE_CHANGE")

"""##  'AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY',       'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON',       'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR'

Number of enquiries to Credit Bureau about the client _____ hour before application
"""

AMT_REQ = ['AMT_REQ_CREDIT_BUREAU_HOUR','AMT_REQ_CREDIT_BUREAU_DAY','AMT_REQ_CREDIT_BUREAU_WEEK','AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR']

missing_data(application_train[AMT_REQ])

application_train[AMT_REQ]=application_train[AMT_REQ].fillna(0)
application_test[AMT_REQ]=application_train[AMT_REQ].fillna(0)

AVG = [ 'APARTMENTS_AVG', 'BASEMENTAREA_AVG', 'YEARS_BEGINEXPLUATATION_AVG','YEARS_BUILD_AVG', 'COMMONAREA_AVG', 'ELEVATORS_AVG', 'ENTRANCES_AVG', 'FLOORSMAX_AVG', 'FLOORSMIN_AVG', 'LANDAREA_AVG',
       'LIVINGAPARTMENTS_AVG', 'LIVINGAREA_AVG', 'NONLIVINGAPARTMENTS_AVG','NONLIVINGAREA_AVG']

MODE = ['APARTMENTS_MODE','BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BUILD_MODE','COMMONAREA_MODE','ELEVATORS_MODE', 'ENTRANCES_MODE', 'FLOORSMAX_MODE', 'FLOORSMIN_MODE',
       'LANDAREA_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE']

MEDI = ['APARTMENTS_MEDI','BASEMENTAREA_MEDI','YEARS_BEGINEXPLUATATION_MEDI','YEARS_BUILD_MEDI','COMMONAREA_MEDI','ELEVATORS_MEDI','ENTRANCES_MEDI','FLOORSMAX_MEDI',
       'FLOORSMIN_MEDI','LANDAREA_MEDI','LIVINGAPARTMENTS_MEDI','LIVINGAREA_MEDI','NONLIVINGAPARTMENTS_MEDI','NONLIVINGAREA_MEDI']

missing_data(application_train[AVG])

missing_data(application_train[MODE])

missing_data(application_train[MEDI])

obj_type  = application_train.dtypes[application_train.dtypes=='object'].index
float_type  = application_train.dtypes[application_train.dtypes=='float64'].index
int_type  = application_train.dtypes[application_train.dtypes=='int64'].index

missing_data(application_train[float_type]).head(50)

# application_train.to_csv("/content/drive/MyDrive/Home Credit/preprocessed_data/app_train.csv",index=False)
# application_test.to_csv("/content/drive/MyDrive/Home Credit/preprocessed_data/app_test.csv",index=False)

"""## CREDIT_CARD"""

credit_df = pd.read_csv("/content/drive/MyDrive/Home Credit/Data/credit_card_balance.csv");

credit_features_train = application_train[["SK_ID_CURR","TARGET"]]
credit_features_test =pd.DataFrame(application_test["SK_ID_CURR"])

credit_df.dtypes.value_counts()

def plot_kde_2(col):
    plt.suptitle(col, fontsize=30)
     
    sns.kdeplot(credit_features_train.loc[credit_features_train['TARGET'] == 0, col],label='TARGET=0')
    sns.kdeplot(credit_features_train.loc[credit_features_train['TARGET'] == 1, col],label='TARGET=1')
    plt.legend(['TARGET(0)', 'TARGET(1)'])
    plt.show()
def plot_hist_2(col):
    plt.suptitle(col, fontsize=30)
    
    credit_features_train.loc[credit_features_train['TARGET'] == 0, col].hist( )
    credit_features_train.loc[credit_features_train['TARGET'] == 1, col].hist( )
    plt.legend(['TARGET(0)', 'TARGET(1)'])
    plt.show()
def kde_hist_2(col):
    plt.suptitle(col, fontsize=30)
    fig, ax = plt.subplots(1, 2, figsize=(16, 8))
    sns.kdeplot(credit_features_train.loc[credit_features_train['TARGET'] == 0,col], ax=ax[0], label='TARGET(0)')
    sns.kdeplot(credit_features_train.loc[credit_features_train['TARGET'] == 1,col], ax=ax[0], label='TARGET(1)')
    ax[0].set_title('KDE plot')
    ax[1].set_title('Histogram plot')
    credit_features_train.loc[credit_features_train['TARGET'] == 0, col].hist(ax=ax[1])
    credit_features_train.loc[credit_features_train["TARGET"] == 1, col].hist(ax=ax[1])
    ax[1].legend(['TARGET(0)', 'TARGET(1)'])

    plt.show()
def plot_count_2(col):
    sns.countplot(x=col, data=credit_features_train)

missing_data(credit_df).head(23)

obj_type  = credit_df.dtypes[credit_df.dtypes=='object'].index
float_type  = credit_df.dtypes[credit_df.dtypes=='float64'].index
int_type  = credit_df.dtypes[credit_df.dtypes=='int64'].index

credit_df[obj_type].head()

"""## NO of previous loans per coutomer"""

NO_LOANS = credit_df.groupby(by = ['SK_ID_CURR'])['SK_ID_PREV'].nunique().reset_index().rename(index = str, columns = {'SK_ID_PREV': 'NO_LOANS'})

NO_LOANS["NO_LOANS"].value_counts()

credit_features_train = credit_features_train.merge(NO_LOANS,on=["SK_ID_CURR"],how="left")
credit_features_test = credit_features_test.merge(NO_LOANS,on=["SK_ID_CURR"],how="left")
print(credit_features_train.shape)
print(credit_features_test.shape)

credit_features_train["NO_LOANS"].value_counts()

credit_features_train["NO_LOANS"].fillna(0,inplace=True)
credit_features_test["NO_LOANS"].fillna(0,inplace=True)

fig = plt.figure(figsize =(15, 3)) 
plt.subplot(1,2,1)
plt.pie(credit_features_train["NO_LOANS"].value_counts(),colors=['C1','C2','C3','C4','C5'], labels = ["NO_LOANS=0","NO_LOANS=1","NO_LOANS=2","NO_LOANS=3","NO_LOANS=4"]) 

plt.subplot(1,2,2)
sns.countplot(x="NO_LOANS",hue="TARGET",palette = "Set2",data=credit_features_train)
plt.tight_layout()
plt.show()

credit_df[int_type].head()

missing_data(credit_df[int_type])

AVG_DPD = credit_df.groupby(by= ['SK_ID_CURR'])['SK_DPD'].mean().reset_index().rename(index = str, columns = {'SK_DPD': 'AVG_DPD'})
credit_features_train = credit_features_train.merge(AVG_DPD, on = ['SK_ID_CURR'], how = 'left')
credit_features_test = credit_features_test.merge(AVG_DPD, on = ['SK_ID_CURR'], how = 'left')

print(credit_features_train.shape)
print(credit_features_test.shape)

credit_features_test.head()

credit_features_train.fillna(0,inplace=True)
credit_features_test.fillna(0,inplace=True)

missing_data(credit_df[float_type])

credit_df[float_type].head()

"""## NO OF INSTALMENTS PAID BY CUSTOMER PER LOAN
CNT_INSTALMENT_MATURE_CUM : Gives Number of paid installments on the previous credit.
"""

grp = credit_df.groupby(by = ['SK_ID_CURR', 'SK_ID_PREV'])['CNT_INSTALMENT_MATURE_CUM'].max().reset_index().rename(index = str, columns = {'CNT_INSTALMENT_MATURE_CUM': 'NO_INSTALMENTS'})
grp1 = grp.groupby(by = ['SK_ID_CURR'])['NO_INSTALMENTS'].sum().reset_index().rename(index = str, columns = {'NO_INSTALMENTS': 'TOTAL_INSTALMENTS'})
credit_features_train = credit_features_train.merge(grp1,on = ['SK_ID_CURR'], how = 'left')
credit_features_test = credit_features_test.merge(grp1,on=['SK_ID_CURR'],how='left')

credit_features_train.fillna(0,inplace=True)
credit_features_test.fillna(0,inplace=True)

print(credit_features_train.shape)
print(credit_features_test.shape)

credit_features_train.drop(["SK_ID_CURR","TARGET"],axis=1,inplace=True)
credit_features_test.drop(["SK_ID_CURR"],axis=1,inplace=True)

application_train=pd.concat([application_train, credit_features_train], axis=1)
application_test=pd.concat([application_test, credit_features_test],axis=1)

del credit_df
del credit_features_test
del credit_features_train

"""## installments"""

installments = pd.read_csv("/content/drive/MyDrive/Home Credit/Data/installments_payments.csv")

installment_train = application_train[["SK_ID_CURR","TARGET"]]
installment_test =pd.DataFrame(application_test["SK_ID_CURR"])

installments.shape

missing_data(installments)

installments.fillna(0,inplace=True)

installments['Days_Extra_Taken']=installments['DAYS_INSTALMENT']-installments['DAYS_ENTRY_PAYMENT']
installments['AMT_INSTALMENT_difference']=installments['AMT_INSTALMENT']-installments['AMT_PAYMENT']

installments.drop(["DAYS_INSTALMENT","AMT_INSTALMENT"],axis=1,inplace=True)
installments.drop(["DAYS_ENTRY_PAYMENT","AMT_PAYMENT"],axis=1,inplace=True)

temp =  installments.drop(["NUM_INSTALMENT_VERSION","NUM_INSTALMENT_NUMBER"],axis=1)

grp = temp.groupby(["SK_ID_CURR"])["Days_Extra_Taken","AMT_INSTALMENT_difference"].max().reset_index().rename(index = str, columns = {"Days_Extra_Taken": 'MAX_Days_Extra_Taken',"AMT_INSTALMENT_difference":"MAX_AMT_INSTALMENT_difference"})

grp["MAX_Days_Extra_Taken"].describe()

installment_train = installment_train.merge(grp,on=["SK_ID_CURR"],how="left")
installment_test = installment_test.merge(grp,on=["SK_ID_CURR"],how="left")

del temp

del grp
del grp1

print(installment_test.shape)
print(installment_train.shape)

installment_test.fillna(0,inplace=True)
installment_train.fillna(0,inplace=True)

missing_data(installment_train)

installment_train.drop(["SK_ID_CURR","TARGET"],axis=1,inplace=True)
installment_test.drop(["SK_ID_CURR"],axis=1,inplace=True)

application_train=pd.concat([application_train, installment_train],axis=1)
application_test=pd.concat([application_test, installment_test], axis=1)

print(application_train.shape)
print(application_test.shape)

del installment_test
del installment_train
del installments

"""##POS_CASH :"""

pos_cash = pd.read_csv("/content/drive/MyDrive/Home Credit/Data/POS_CASH_balance.csv")

pos_train = application_train[["SK_ID_CURR","TARGET"]]
pos_test =pd.DataFrame(application_test["SK_ID_CURR"])

temp1 = pos_cash.groupby(by= ['SK_ID_CURR'])['SK_DPD'].mean().reset_index().rename(index = str, columns = {'SK_DPD': 'AVG_DPD'})
temp2 = pos_cash.groupby(by= ['SK_ID_CURR'])['SK_DPD_DEF'].mean().reset_index().rename(index = str, columns = {'SK_DPD_DEF': 'AVG_DPD_DEF'})

pos_train = pos_train.merge(temp1, on = ['SK_ID_CURR'], how = 'left')
pos_test = pos_test.merge(temp1, on = ['SK_ID_CURR'], how = 'left')

pos_train = pos_train.merge(temp2, on = ['SK_ID_CURR'], how = 'left')
pos_test = pos_test.merge(temp2, on = ['SK_ID_CURR'], how = 'left')

del temp1
del temp2

"""# CNT_INSTALMENT_MATURE_CUM : Gives  Number of paid installments on the previous credit."""

grp = pos_cash.groupby(by = ['SK_ID_CURR', 'SK_ID_PREV'])['CNT_INSTALMENT'].max().reset_index().rename(index = str, columns = {'CNT_INSTALMENT': 'POS_PAID_INSTALMENTS'})
grp1 = grp.groupby(by = ['SK_ID_CURR'])['POS_PAID_INSTALMENTS'].sum().reset_index().rename(index = str, columns = {'POS_PAID_INSTALMENTS': 'POS_TOTAL_PAID_INSTALMENTS'})
grp2 = pos_cash.groupby(by = ['SK_ID_CURR', 'SK_ID_PREV'])['CNT_INSTALMENT_FUTURE'].min().reset_index().rename(index = str, columns = {'CNT_INSTALMENT_FUTURE': 'POS_NOTPAID_INSTALMENTS'})
grp3 = grp2.groupby(by = ['SK_ID_CURR'])['POS_NOTPAID_INSTALMENTS'].sum().reset_index().rename(index = str, columns = {'POS_NOTPAID_INSTALMENTS': 'POS_TOTAL_NOTPAID_INSTALMENTS'})

pos_train = pos_train.merge(grp1, on = ['SK_ID_CURR'], how = 'left')
pos_test = pos_test.merge(grp1, on = ['SK_ID_CURR'], how = 'left')
pos_train = pos_train.merge(grp3, on = ['SK_ID_CURR'], how = 'left')
pos_test = pos_test.merge(grp3, on = ['SK_ID_CURR'], how = 'left')

del grp1
del grp2
del grp3
del grp

POS_NO_LOANS = pos_cash.groupby(by = ['SK_ID_CURR'])['SK_ID_PREV'].nunique().reset_index().rename(index = str, columns = {'SK_ID_PREV': 'NO_LOANS'})

pos_train = pos_train.merge(POS_NO_LOANS, on = ['SK_ID_CURR'], how = 'left')
pos_test = pos_test.merge(POS_NO_LOANS, on = ['SK_ID_CURR'], how = 'left')

del POS_NO_LOANS

print(pos_train.shape)
print(pos_test.shape)

pos_train.fillna(0,inplace=True)
pos_test.fillna(0,inplace=True)

pos_train.drop(["SK_ID_CURR","TARGET"],axis=1,inplace=True)
pos_test.drop(["SK_ID_CURR"],axis=1,inplace=True)

application_train=pd.concat([application_train, pos_train],axis=1)
application_test=pd.concat([application_test, pos_test], axis=1)

del pos_cash
del pos_train
del pos_test

print(application_train.shape)
print(application_test.shape)

"""## bureau"""

b1=pd.read_csv("/content/drive/MyDrive/Home Credit/Data/bureau.csv")

missing_values=b1.isnull().sum()
print(missing_values)

# b1=b

obj_type  = b1.dtypes[b1.dtypes=='object'].index
float_type  = b1.dtypes[b1.dtypes=='float64'].index
int_type  = b1.dtypes[b1.dtypes=='int64'].index

grpby=b1.groupby(['SK_ID_CURR'])
ftrs = pd.DataFrame({'SK_ID_CURR': b1['SK_ID_CURR'].unique()})
ftrs.head()

del grpby

b1['bureau_credit_active_binary'] = (b1['CREDIT_ACTIVE'] != 'Closed').astype(int)
b1['bureau_credit_enddate_binary'] = (b1['DAYS_CREDIT_ENDDATE'] > 0).astype(int)
groupby_SK_ID_CURR = b1.groupby(by=['SK_ID_CURR'])

group_object = groupby_SK_ID_CURR['DAYS_CREDIT'].agg('count').reset_index()
#When we reset the index, the old index is added as a column, and a new sequential index is used
group_object.rename(index=str, columns={'DAYS_CREDIT': 'bureau_number_of_past_loans'},inplace=True)
ftrs = ftrs.merge(group_object, on=['SK_ID_CURR'], how='left')
ftrs.head()

group_object = groupby_SK_ID_CURR['CREDIT_TYPE'].agg('nunique').reset_index()
group_object.rename(index=str, columns={'CREDIT_TYPE': 'bureau_number_of_loan_types'},inplace=True)

ftrs = ftrs.merge(group_object, on=['SK_ID_CURR'], how='left')
ftrs.head()

ftrs['bureau_average_of_past_loans_per_type'] = \
    ftrs['bureau_number_of_past_loans'] / ftrs['bureau_number_of_loan_types']
ftrs = ftrs.replace([np.inf, -np.inf], np.nan)
ftrs.fillna(value=0, axis=1, inplace=True)

group_object = groupby_SK_ID_CURR['bureau_credit_active_binary'].agg('mean').reset_index()

ftrs = ftrs.merge(group_object, on=['SK_ID_CURR'], how='left')

group_object = groupby_SK_ID_CURR['AMT_CREDIT_SUM_DEBT'].agg('sum').reset_index()
group_object.rename(index=str, columns={'AMT_CREDIT_SUM_DEBT': 'bureau_total_customer_debt'},inplace=True)

ftrs = ftrs.merge(group_object, on=['SK_ID_CURR'], how='left')
ftrs.head()

group_object = groupby_SK_ID_CURR['AMT_CREDIT_SUM'].agg('sum').reset_index()
group_object.rename(index=str, columns={'AMT_CREDIT_SUM': 'bureau_total_customer_credit'},inplace=True)

ftrs = ftrs.merge(group_object, on=['SK_ID_CURR'], how='left')

ftrs['bureau_debt_credit_ratio'] = \
    ftrs['bureau_total_customer_debt'] / ftrs['bureau_total_customer_credit']
    
ftrs = ftrs.replace([np.inf, -np.inf], np.nan)
ftrs['bureau_debt_credit_ratio'].fillna(value=0, axis=0, inplace=True)

group_object = groupby_SK_ID_CURR['AMT_CREDIT_SUM_OVERDUE'].agg('sum').reset_index()
group_object.rename(index=str, columns={'AMT_CREDIT_SUM_OVERDUE': 'bureau_total_customer_overdue'},inplace=True)

ftrs = ftrs.merge(group_object, on=['SK_ID_CURR'], how='left')
ftrs.head()

ftrs['bureau_overdue_debt_ratio'] = \
    ftrs['bureau_total_customer_overdue'] / ftrs['bureau_total_customer_debt']

ftrs = ftrs.replace([np.inf, -np.inf], np.nan)
ftrs['bureau_overdue_debt_ratio'].fillna(value=0, axis=0, inplace=True)

b1[~b1['AMT_CREDIT_SUM_LIMIT'].isnull()][0:2]

b1['AMT_CREDIT_SUM_DEBT'] = b1['AMT_CREDIT_SUM_DEBT'].fillna(0)
b1['AMT_CREDIT_SUM'] = b1['AMT_CREDIT_SUM'].fillna(0)

grp1 = b1[['SK_ID_CURR', 'AMT_CREDIT_SUM_DEBT']].groupby(by = ['SK_ID_CURR'])['AMT_CREDIT_SUM_DEBT'].sum().reset_index().rename( index = str, columns = { 'AMT_CREDIT_SUM_DEBT': 'TOTAL_CUSTOMER_DEBT'})
grp2 = b1[['SK_ID_CURR', 'AMT_CREDIT_SUM']].groupby(by = ['SK_ID_CURR'])['AMT_CREDIT_SUM'].sum().reset_index().rename( index = str, columns = { 'AMT_CREDIT_SUM': 'TOTAL_CUSTOMER_CREDIT'})
grp1

ftrs=ftrs.merge(grp1, on = ['SK_ID_CURR'], how = 'left')
ftrs=ftrs.merge(grp2, on = ['SK_ID_CURR'], how = 'left')
ftrs.head()

ftrs['DEBT_CREDIT_RATIO'] = ftrs['TOTAL_CUSTOMER_DEBT']/ftrs['TOTAL_CUSTOMER_CREDIT']

ftrs = ftrs.replace([np.inf, -np.inf], 0)

group_object = groupby_SK_ID_CURR['CNT_CREDIT_PROLONG'].agg('sum').reset_index()
group_object.rename(index=str, columns={'CNT_CREDIT_PROLONG': 'bureau_total_prolonged_count'},inplace=True)

ftrs = ftrs.merge(group_object, on=['SK_ID_CURR'], how='left')

group_object = groupby_SK_ID_CURR['bureau_credit_enddate_binary'].agg('mean').reset_index()
group_object.rename(index=str, columns={'bureau_credit_enddate_binary': 'bureau_credit_enddate_percentage'},inplace=True)

ftrs = ftrs.merge(group_object, on=['SK_ID_CURR'], how='left')

group_object = groupby_SK_ID_CURR['SK_ID_BUREAU'].agg('nunique').reset_index()
group_object.rename(index=str, columns={'SK_ID_BUREAU': 'bureau_query_count'},inplace=True)

ftrs = ftrs.merge(group_object, on=['SK_ID_CURR'], how='left')

b1['CREDIT_ENDDATE_BINARY'] = b1['DAYS_CREDIT_ENDDATE']

def f(x):
    if x<0:
        y = 0
    else:
        y = 1   
    return y

b1['CREDIT_ENDDATE_BINARY'] = b1.apply(lambda x: f(x.DAYS_CREDIT_ENDDATE), axis = 1)
print("New Binary Column calculated")

grp = b1.groupby(by = ['SK_ID_CURR'])['CREDIT_ENDDATE_BINARY'].mean().reset_index().rename(index=str, columns={'CREDIT_ENDDATE_BINARY': 'CREDIT_ENDDATE_PERCENTAGE'})
ftrs=ftrs.merge(grp, on = ['SK_ID_CURR'], how = 'left')
ftrs.head()

"""## bureau balance 
less features are  present
"""

bur=pd.read_csv("/content/drive/MyDrive/Home Credit/Data/bureau_balance.csv")

bur.head()

bur = bur.merge(b1[['SK_ID_CURR', 'SK_ID_BUREAU']], on='SK_ID_BUREAU', how='right')
bur.head

grpby1=bur.groupby(['SK_ID_CURR'])
ftrs1 = pd.DataFrame({'SK_ID_CURR': bur['SK_ID_CURR'].unique()})
ftrs1.head()

grpby2=bur.groupby(['SK_ID_CURR'])
ftrs2 = pd.DataFrame({'SK_ID_BUREAU': bur['SK_ID_BUREAU'].unique()})

groupby_SK_ID_CURR1 = bur.groupby(by=['SK_ID_CURR'])

group_object = groupby_SK_ID_CURR1['MONTHS_BALANCE'].agg('sum').reset_index()
#When we reset the index, the old index is added as a column, and a new sequential index is used
group_object.rename(index=str, columns={'MONTHS_BALANCE': 'Month_bal_rel_to_appctn_date'},inplace=True)
ftrs1 = ftrs1.merge(group_object, on=['SK_ID_CURR'], how='left')
ftrs1.head()

def status_to_int(status):
    if status in ['X', 'C']:
        return 0
    if pd.isnull(status):
        return np.nan
    return int(status)

bur['bureau_balance_dpd_level'] = bur['STATUS'].apply(status_to_int)
bur['bureau_balance_status_unknown'] = (bur['STATUS'] == 'X').astype(int)
bur['bureau_balance_status_closed'] = (bur['STATUS'] == 'C').astype(int)

group_object = groupby_SK_ID_CURR1['bureau_balance_dpd_level'].agg('sum').reset_index()
#When we reset the index, the old index is added as a column, and a new sequential index is used
group_object.rename(index=str, columns={'bureau_balance_dpd_level': 'debt_past_due'},inplace=True)
ftrs1 = ftrs1.merge(group_object, on=['SK_ID_CURR'], how='left')
ftrs1.head()

group_object = groupby_SK_ID_CURR1['bureau_balance_status_closed'].agg('max').reset_index()
#When we reset the index, the old index is added as a column, and a new sequential index is used
group_object.rename(index=str, columns={'bureau_balance_status_closed': 'bb_status_closed'},inplace=True)
ftrs1 = ftrs1.merge(group_object, on=['SK_ID_CURR'], how='left')
ftrs1.head()

bbData=pd.DataFrame()
bbData=ftrs.merge(ftrs1,left_on=['SK_ID_CURR'], right_on=['SK_ID_CURR'], how='left', validate='one_to_one')
bbData.head()

bbData.head()

application_train = application_train.merge(bbData,on=["SK_ID_CURR"],how="left")
application_test = application_test.merge(bbData,on=["SK_ID_CURR"],how="left")

print(application_train.shape)
print(application_test.shape)

del bur
del b1
del grpby1
del grpby2
del groupby_SK_ID_CURR
del groupby_SK_ID_CURR1
del group_object

del ftrs1
del ftrs
del ftrs2

"""## Previous_Application"""

prev=pd.read_csv("/content/drive/MyDrive/Home Credit/Data/previous_application.csv")

# PREV_APP_AGG = []
# for agg in ['mean', 'min', 'max', 'sum', 'var']:
#     for select in ['AMT_ANNUITY',
#                    'AMT_APPLICATION',
#                    'AMT_CREDIT',
#                    'AMT_DOWN_PAYMENT',
#                    'AMT_GOODS_PRICE',
#                    'CNT_PAYMENT',
#                    'DAYS_DECISION',
#                    'HOUR_APPR_PROCESS_START',
#                    'RATE_DOWN_PAYMENT'
#                    ]:
#         PREV_APP_AGG.append((select, agg))
# PREV_APP_AGG = [(['SK_ID_CURR'], PREV_APP_AGG)]

# grpby_agg_names = []
# for groupby_cols, specs in tqdm(PREV_APP_AGG):
#     group_object = prev.groupby(groupby_cols)
#     for select, agg in tqdm(specs):
#         grpby_agg_name = '{}_{}_{}'.format('_'.join(groupby_cols), agg, select)
#         application_train = application_train.merge(group_object[select]
#                               .agg(agg)
#                               .reset_index()
#                               .rename(index=str,
#                                       columns={select: grpby_agg_name})
#                               [groupby_cols + [grpby_agg_name]],
#                               on=groupby_cols,
#                               how='left')
#         application_test = application_test.merge(group_object[select]
#                               .agg(agg)
#                               .reset_index()
#                               .rename(index=str,
#                                       columns={select: grpby_agg_name})
#                               [groupby_cols + [grpby_agg_name]],
#                               on=groupby_cols,
#                               how='left')
#         grpby_agg_names.append(grpby_agg_name)

ftrs = pd.DataFrame({'SK_ID_CURR': prev['SK_ID_CURR'].unique()})
ftrs.head()

prev_sorted = prev.sort_values(['SK_ID_CURR', 'DAYS_DECISION'])
prev_sorted.head()

group_object = prev_sorted.groupby(by=['SK_ID_CURR'])['SK_ID_PREV'].nunique().reset_index()
group_object.rename(index=str,
                    columns={'SK_ID_PREV': 'previous_application_number_of_prev_application'},
                    inplace=True)
ftrs = ftrs.merge(group_object, on=['SK_ID_CURR'], how='left')

prev_sorted['previous_application_prev_was_approved'] = (prev_sorted['NAME_CONTRACT_STATUS'] == 'Approved').astype('int')
group_object = prev_sorted.groupby(by=['SK_ID_CURR'])['previous_application_prev_was_approved'].last().reset_index()
ftrs = ftrs.merge(group_object, on=['SK_ID_CURR'], how='left')
ftrs.head(5)

prev_sorted['previous_application_prev_was_refused'] = (prev_sorted['NAME_CONTRACT_STATUS'] == 'Refused').astype('int')
group_object = prev_sorted.groupby(by=['SK_ID_CURR'])['previous_application_prev_was_refused'].last().reset_index()
ftrs = ftrs.merge(group_object, on=['SK_ID_CURR'], how='left')
ftrs.head(5)

numbers_of_applications=[1,3,5]

for number in numbers_of_applications:
    prev_tail = prev_sorted.groupby(by=['SK_ID_CURR']).tail(number)

    group_object = prev_tail.groupby(by=['SK_ID_CURR'])['CNT_PAYMENT'].mean().reset_index()
    group_object.rename(index=str, columns={
        'CNT_PAYMENT': 'previous_application_term_of_last_{}_credits_mean'.format(number)},
                        inplace=True)
    ftrs = ftrs.merge(group_object, on=['SK_ID_CURR'], how='left')

    group_object = prev_tail.groupby(by=['SK_ID_CURR'])['DAYS_DECISION'].mean().reset_index()
    group_object.rename(index=str, columns={
        'DAYS_DECISION': 'previous_application_days_decision_about_last_{}_credits_mean'.format(number)},
                        inplace=True)
    ftrs = ftrs.merge(group_object, on=['SK_ID_CURR'], how='left')

    group_object = prev_tail.groupby(by=['SK_ID_CURR'])['DAYS_FIRST_DRAWING'].mean().reset_index()
    group_object.rename(index=str, columns={
        'DAYS_FIRST_DRAWING': 'previous_application_days_first_drawing_last_{}_credits_mean'.format(number)},
                        inplace=True)
    ftrs = ftrs.merge(group_object, on=['SK_ID_CURR'], how='left')
ftrs.head(5)

prev_sorted['revolving_loan'] = (prev_sorted['NAME_CONTRACT_TYPE'] == 'Revolving loans').astype('int')
prev_sorted['cash_loan'] = (prev_sorted['NAME_CONTRACT_TYPE'] == 'Cash loans').astype('int')
prev_sorted['consumer_loan'] = (prev_sorted['NAME_CONTRACT_TYPE'] == 'Consumer loans').astype('int')
prev_sorted.head(5)

prev_sorted['credit_goods_diff'] = prev_sorted['AMT_CREDIT'] - prev_sorted['AMT_GOODS_PRICE']
prev_sorted['credit_goods_ratio'] = prev_sorted['AMT_CREDIT'] / prev_sorted['AMT_GOODS_PRICE']

prev_sorted['application_credit_diff'] = prev_sorted['AMT_APPLICATION'] - prev_sorted['AMT_CREDIT']
prev_sorted['application_credit_ratio'] = prev_sorted['AMT_APPLICATION'] / prev_sorted['AMT_CREDIT']

prev_sorted['NAME_PRODUCT_TYPE_x_sell'] = (prev_sorted['NAME_PRODUCT_TYPE'] == 'x-sell').astype('int')
prev_sorted['NAME_PRODUCT_TYPE_walk_in'] = (prev_sorted['NAME_PRODUCT_TYPE'] == 'walk-in').astype('int')

prev_sorted['NAME_PAYMENT_TYPE_account'] = (prev_sorted['NAME_PAYMENT_TYPE'] == 'Non-cash from your account').astype('int')
prev_sorted['NAME_PAYMENT_TYPE_employer'] = (prev_sorted['NAME_PAYMENT_TYPE'] == 'Cashless from the account of the employer').astype('int')
prev_sorted.head(5)

prev_sorted['approved'] = (prev_sorted['NAME_CONTRACT_STATUS'] == 'Approved').astype('int')
prev_sorted['refused'] = (prev_sorted['NAME_CONTRACT_STATUS'] == 'Refused').astype('int')

prev_sorted_grpby = prev_sorted.groupby(by=['SK_ID_CURR'])

g = prev_sorted_grpby['refused'].mean().reset_index()
g.rename(index=str, columns={'refused': 'previous_application_fraction_of_refused_applications'}, inplace=True)
ftrs = ftrs.merge(g, on=['SK_ID_CURR'], how='left')
ftrs.head(5)

ftrs.shape

application_train=application_train.merge(ftrs,left_on=['SK_ID_CURR'],right_on=['SK_ID_CURR'], how='left',validate='one_to_one')
application_test=application_test.merge(ftrs,left_on=['SK_ID_CURR'],right_on=['SK_ID_CURR'], how='left',validate='one_to_one')



del prev
del prev_sorted
del prev_sorted_grpby
del ftrs

print(application_train.shape)
print(application_test.shape)

train_id = application_train["SK_ID_CURR"]
train_target = application_train["TARGET"]
test_id = application_test["SK_ID_CURR"]

application_train.drop(["SK_ID_CURR","TARGET"],axis=1,inplace=True)
application_test.drop(["SK_ID_CURR"],axis=1,inplace=True)

print(application_train.shape)
print(application_test.shape)

from sklearn.impute import SimpleImputer 
imputer = SimpleImputer(missing_values=np.nan, strategy='median')
imputer.fit(application_train)
application_train = imputer.transform(application_train)
application_test = imputer.transform(application_test)

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range = (0, 1))
scaler.fit(application_train)
application_train = scaler.transform(application_train)
application_test = scaler.transform(application_test)

application_train = pd.DataFrame(application_train)
application_test = pd.DataFrame(application_test)

# application_train.to_csv("/content/drive/MyDrive/Home Credit/final_Data/train.csv")
# application_test.to_csv("/content/drive/MyDrive/Home Credit/final_Data/test.csv")

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import BaggingClassifier
from sklearn import tree
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
 
 
import xgboost as xgb
import lightgbm as lgb
 
 
 
from sklearn.metrics import accuracy_score
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import confusion_matrix, roc_auc_score ,roc_curve,auc

"""Lgb"""



from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(application_train,train_target, test_size = 0.3, random_state = 0)

import lightgbm as lgb
model4 = lgb.LGBMClassifier()
model4.fit(X_train, y_train)

y_pred_lgb=model4.predict(X_test)

from sklearn.metrics import accuracy_score
accuracy=accuracy_score(y_pred_lgb, y_test)
print('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test, y_pred_lgb)))

model4.fit(application_train,train_target)

y_pred_lgb=model4.predict_proba(application_test)[:,1]

"""xgb"""

model2=xgb.XGBClassifier(random_state=1,learning_rate=0.1,n_estimators=800,max_depth=11,min_child_weight=9,subsample=1,gamma=0.2,colsample_bytree=0.4,nthread=-1,objective='binary:logistic',scale_pos_weight=1,reg_alpha=0.6,reg_lambda=3,seed=42)
model2.fit(application_train, train_target)

y_pred_xgb = model2.predict_proba(application_test)[:, 1]

"""random forest"""

clf = RandomForestClassifier(n_estimators=400, min_samples_split=10, min_samples_leaf=5, n_jobs=-1, random_state=42)
clf.fit(application_train,train_target)
y_pred_rf = clf.predict_proba(application_test)[:, 1]

weighted_prediction = pd.DataFrame()
weighted_prediction["SK_ID_CURR"] = test_id

weighted_prediction["TARGET"] = (0.20*xgb_pred)+(0.70*lgb_pred)+(0.10*rf_pred)

# weighted_prediction.to_csv('/content/drive/MyDrive/Home Credit/final_Data/final_submit2.csv',index=False)